"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[1477],{10:e=>{e.exports=JSON.parse('{"blogPosts":[{"id":"yaml-based-vivado-hardware-project","metadata":{"permalink":"/blog/yaml-based-vivado-hardware-project","editUrl":"https://github.com/syed-ahmed/syed-ahmed.github.io/blob/main/website/blog/2020-06-02-yaml-based-vivado-hardware-project/index.md?plain=1","source":"@site/blog/2020-06-02-yaml-based-vivado-hardware-project/index.md","title":"Creating Vivado Hardware Platform using YAML","description":"The hardware design community have been following a design paradigm where you think of designing hardware spatially, i.e. start with a block diagram. I remember from my undergraduate logic design course, a block diagram was required before we started a lab assignment. It is the right approach for designing hardware. Vivado reinforces this paradigm by asking people to open the Vivado GUI, use the IP integrator and create a block design.","date":"2020-06-02T00:00:00.000Z","formattedDate":"June 2, 2020","tags":[{"label":"vivado","permalink":"/blog/tags/vivado"},{"label":"fpga","permalink":"/blog/tags/fpga"},{"label":"tcl","permalink":"/blog/tags/tcl"},{"label":"yaml","permalink":"/blog/tags/yaml"},{"label":"hydra","permalink":"/blog/tags/hydra"}],"readingTime":5.59,"truncated":true,"authors":[{"name":"Syed Tousif Ahmed","title":"Software Engineer","url":"https://syed.sh","imageURL":"https://github.com/syed-ahmed/syed-ahmed.github.io/raw/main/website/static/img/profile-pic1.jpg","key":"syed"}],"frontMatter":{"slug":"yaml-based-vivado-hardware-project","title":"Creating Vivado Hardware Platform using YAML","authors":"syed","tags":["vivado","fpga","tcl","yaml","hydra"]},"nextItem":{"title":"Use All Tunable Vivado Internal Params","permalink":"/blog/vivado-internal-params"}},"content":"The hardware design community have been following a design paradigm where you think of designing hardware spatially, i.e. start with a block diagram. I remember from my undergraduate logic design course, a block diagram was required before we started a lab assignment. It is the right approach for designing hardware. Vivado reinforces this paradigm by asking people to open the Vivado GUI, use the IP integrator and create a block design. \x3c!--truncate--\x3e\\n\\nAn alternative to the GUI are the Vivado TCL commands. <img style={{float: \'right\', height: 1000}} src={require(\'./assets/flow.png\').default}/>TCL is the scripting language of the hardware design community. If you wanted to generate block designs based on some input parameters, you would be using TCL. If you are developing on a headless server, you would have no other choice than to use TCL.\\nTCL programming is not bad, however, I\'m very used to Python as my scripting language of choice. If you wanted to generate a block design that was data driven (for instance, use the decisions made by a PyTorch ML model or a Google OR-Tools model), you would need to introduce Python into the picture because TCL doesn\'t have such packages.\\nIf Python was a front-end for Vivado, our problem would have been solved! So how do we go from Python to TCL? People follow different approaches. You could write a Python script that generates a TCL file with the desired commands and parameters. You could use [elmer](http://elmer.sourceforge.net/examples.html) to call Python from TCL (for instance, fork into a python class, get values from it and feed it to your TCL command), but elmer doesn\'t work for Python3. While writing this post, I came to know about Olof Kindgren\'s [edalize](https://github.com/olofk/edalize) project! I\'m taking a similar approach to this project, i.e. have a templated TCL script and use Python to feed those template parameters.\\nI generate a `yaml` configuration file from python and parse it in a TCL script. TCL has a yaml parser and can parse it to a TCL dictionary. You can then use the values from the dictionary in generic TCL functions. I discovered this [repository](https://github.com/Xilinx/wireless-apps/tree/master/scripts) from Xilinx which uses a yaml configuration file in a TCL script to generate a vivado project. In addition, I found this excellent [Vivado hardware platform project](https://github.com/Xilinx/Vitis-AI/tree/master/DPU-TRD/prj/Vivado/scripts) from Xilinx that uses TCL dicts to fully parameterize the generation of a block design. Combining these two, I have the approach on the right hand side.\\n\\nConsider the following `yaml` file for one of our projects:\\n\\n```\\ndict_prj:\\n  dict_sys:\\n    bd_name: floorplan_static\\n    bd_ooc: Hierarchical\\n    prj_board: em.avnet.com:ultra96v2:part0:1.0\\n    prj_name: floorplan_static\\n    prj_part: xczu3eg-sbva484-1-e\\n  dict_stgy:\\n    synth:\\n      synth_1:\\n        STRATEGY: \\"\\"\\n        JOBS: 40\\n    impl:\\n      impl_1_01:\\n        PARENT: \\"\\"\\n        STRATEGY: \\"\\"\\n        JOBS: 40\\n      impl_1_03:\\n        PARENT: \\"\\"\\n        STRATEGY: \\"\\"\\n        JOBS: 40\\n      impl_1_14:\\n        PARENT: \\"\\"\\n        STRATEGY: \\"\\"\\n        JOBS: 40\\n      impl_1_15:\\n        PARENT: \\"\\"\\n        STRATEGY: \\"\\"\\n        JOBS: 40\\n  dict_xdc:\\n    -\\n      NAME: pblocks.xdc\\n      IS_TARGET: True\\n  dict_src:\\n    -\\n      NAME: AxiLite2Bft_v2_0.v\\n    -\\n      NAME: converter.v\\n      ...\\n  dict_hier:\\n    h_bft:\\n      PATH: bft\\n  dict_ip:\\n    ip_ps:\\n      PATH: zynq_ultra_ps_e_0\\n      NAME: zynq_ultra_ps_e\\n      VLNV: \\"\\"\\n      PROP:\\n        PSU__CRL_APB__PL1_REF_CTRL__ACT_FREQMHZ: \\"19.660831\\"\\n        PSU__FPGA_PL1_ENABLE: \\"1\\"\\n        PSU__CRL_APB__PL1_REF_CTRL__SRCSEL: RPLL\\n        PSU__PL_CLK1_BUF: \\"TRUE\\"\\n        PSU__SAXIGP2__DATA_WIDTH: \\"32\\"\\n        PSU__USE__S_AXI_GP2: 1\\n      BACFG:\\n        apply_board_preset: 1\\n    ip_clk_rst_usr:\\n      PATH: rst_ps8_0_100M\\n      NAME: proc_sys_reset\\n      VLNV: \\"\\"\\n      PROP: \\"\\"\\n      BACFG: \\"\\"\\n      ...\\n  dict_pin:\\n    p_bft_S00_AXI:\\n      CLASS: INTF_PIN\\n      PATH: \\"bft/S00_AXI\\"\\n      MODE: Slave\\n      VLNV: \\"xilinx.com:interface:aximm_rtl:1.0\\"\\n      ...\\n  dict_cn:\\n    cn_overlay:\\n      PIN intf PIN:\\n        zynq_ultra_ps_e_0/S_AXI_HP0_FPD: bft/M00_AXI\\n        zynq_ultra_ps_e_0/M_AXI_HPM0_FPD: bft/S00_AXI\\n        zynq_ultra_ps_e_0/M_AXI_HPM1_FPD: bft/S01_AXI\\n        ...\\n      PIN from PIN:\\n        bft/bft_0/dout_leaf_0: bft/AxiLite2Bft_v2_0_0/host_interface2bft\\n        bft/leaf_interface_0/din_leaf_user2interface: bft/axi_dma_0/m_axis_mm2s_tdata\\n        bft/leaf_interface_0/vld_user2interface: bft/axi_dma_0/m_axis_mm2s_tvalid\\n        bft/xlconcat_0/In0: bft/axi_dma_0/mm2s_introut\\n        bft/xlconcat_0/In1: bft/axi_dma_1/s2mm_introut\\n        ...\\n  dict_addr:\\n    addr_s00_axi:\\n      REG: bft/AxiLite2Bft_v2_0_0/s00_axi/reg0\\n      RANGE: 0x00001000\\n      OFFSET: 0xA0002000\\n    ...\\n```\\n\\n\\nYou can see that we have parameterized part and board numbers, since we want to support multiple boards with the same design. The PS IP is also parameterized since we want to support Zynq 7 series and Zynq UltraScale+. Similarly, you can see synthesis and implementation strategies are now parameterized, which means more design space exploration! Even connections of pins - we can now configure that with a python script if we want to generate a different overlay.\\n\\nFollowing is how we are parsing the yaml in the TCL script:\\n\\n```\\n#****************************************************************\\n# check if file exists\\n#****************************************************************\\nproc lib_check_file { file_name } {\\n    set file_content \\"\\"\\n    if { [ file exists  $file_name ] } {\\n        set fp [open $file_name r]\\n        set file_content [read $fp]\\n        close $fp\\n    } else {\\n        lib_error YAML \\"Cannot open filename $file_name...\\"\\n    }\\n    return $file_content\\n}\\n\\n#****************************************************************\\n# load yaml file into dict\\n#****************************************************************\\nproc lib_yaml2dict { file_name } {\\n    set file_content [ lib_check_file $file_name ]\\n    return [yaml::yaml2dict $file_content]\\n}\\n\\n#****************************************************************\\n# set global dict_prj\\n#****************************************************************\\nset dict_prj  {}\\n\\nset config_file \\"config.yaml\\"\\nset cfg [lib_yaml2dict $config_file]\\nset dict_prj [dict get $cfg dict_prj]\\n```\\n\\nThe rest of the script is then from [Xilinx\'s Vitis-AI repository](https://github.com/Xilinx/Vitis-AI/tree/master/DPU-TRD/prj/Vivado/scripts). \\nNow if you are wondering, why YAML? It\'s solely because of [hydra](https://hydra.cc/). Hydra\'s ability to compose a yaml file from a hierarchy of yaml files is a stunning feature that lets me focus on the parameters that matter in my project, and exactly fits our use case of supporting multiple moving things. For instance, Vivado is not backwards compatible and a set of parameters that worked in `2018.3` might not work in `2019.2`, for each board, Zynq7 and Zynq UltraScale+ PS parameters are different, and so on. Following is what my hydra conf looks like:\\n```\\n\u251c\u2500\u2500 conf\\n\u2502   \u251c\u2500\u2500 architecture\\n|   \u251c\u2500\u2500 board\\n|       \u251c\u2500\u2500 pynq_z1.yaml\\n|       \u251c\u2500\u2500 ultra96v2.yaml\\n|       \u251c\u2500\u2500 zcu102.yaml\\n|       \u251c\u2500\u2500 zed.yaml\\n\u2502   \u251c\u2500\u2500 vivado\\n|       \u251c\u2500\u2500 2018.3\\n|           \u251c\u2500\u2500 ultra96v2.yaml\\n|           \u251c\u2500\u2500 zcu102.yaml\\n|       \u251c\u2500\u2500 2019.2\\n|           \u251c\u2500\u2500 zed.yaml\\n|   \u251c\u2500\u2500 config.yaml\\n\u2514\u2500\u2500 generate_vivado_project.py\\n```\\nI can then execute the `generate_vivado_project.py` script with the command line parameters, such as `board=zed.yaml`, `vivado=2019.2` and it will generate a `config.yaml` which is the composition of the yaml files from the respective `conf` subdirectories. And now you can just source the tcl file with this config file! As a result, block design in vivado becomes a matter of writing a `yaml` file. \\n\\nNow some cons of this approach - I have to find the names of the parameters, such as the Zynq IP parameters, AXI DMA parameters by trial-error in the GUI and then note it down in my configuration. This could be easily mitigated if all these parameters are nicely documented somewhere (or alternatively you could actually finish the design in GUI, export the TCL file and then note down the parameters from that TCL file...). Moreover, I like connecting the input and output of a block in the GUI than manually writing it in the `yaml` file. Hence, the way I see it, if you are prototyping and playing around with ideas, GUI approach is more flexible as I\'m able to see the design like I would have done in my logic design lab assignments, whereas, when doing automation around generating hardware, templated TCL scripts with a configuration file is the way to go :)."},{"id":"vivado-internal-params","metadata":{"permalink":"/blog/vivado-internal-params","editUrl":"https://github.com/syed-ahmed/syed-ahmed.github.io/blob/main/website/blog/2020-03-31-use-all-tunable-vivado-internal-params/index.md?plain=1","source":"@site/blog/2020-03-31-use-all-tunable-vivado-internal-params/index.md","title":"Use All Tunable Vivado Internal Params","description":"So I learned about this parameter that can be set in vivado, called place.debugShape, for my overlay generation work. It was referred in one of the tcl scripts in RapidWright with the command setparam place.debugShape $shapesFileName. Studying the tcl commands guide, I found out, you can list the description of this property using reportparam -nondefault, which suggests place.debugShape is an internal knob. A caveat of the reportparam command is that, it only lists these internal params when it is changed from its default value (you can know its default value by getparam). That is, reportparam -non_default won\'t show you all the internal knobs that are in vivado.","date":"2020-03-31T00:00:00.000Z","formattedDate":"March 31, 2020","tags":[{"label":"vivado","permalink":"/blog/tags/vivado"},{"label":"fpga","permalink":"/blog/tags/fpga"},{"label":"tcl","permalink":"/blog/tags/tcl"}],"readingTime":1.865,"truncated":true,"authors":[{"name":"Syed Tousif Ahmed","title":"Software Engineer","url":"https://syed.sh","imageURL":"https://github.com/syed-ahmed/syed-ahmed.github.io/raw/main/website/static/img/profile-pic1.jpg","key":"syed"}],"frontMatter":{"slug":"vivado-internal-params","title":"Use All Tunable Vivado Internal Params","authors":"syed","tags":["vivado","fpga","tcl"]},"prevItem":{"title":"Creating Vivado Hardware Platform using YAML","permalink":"/blog/yaml-based-vivado-hardware-project"},"nextItem":{"title":"Software Development Workflow with Docker","permalink":"/blog/docker-workflow"}},"content":"So I learned about this parameter that can be set in vivado, called `place.debugShape`, for my overlay generation work. It was referred in one of the tcl scripts in RapidWright with the command `set_param place.debugShape $shapesFileName`. Studying the tcl commands guide, I found out, you can list the description of this property using `report_param -non_default`, which suggests `place.debugShape` is an internal knob. A caveat of the report_param command is that, it only lists these internal params when it is changed from its default value (you can know its default value by `get_param`). That is, `report_param -non_default` won\'t show you all the internal knobs that are in vivado. \x3c!--truncate--\x3e\\n\\nNow that you have some context, it got me thinking are there other useful params which we should know about? Looks like somebody asked the same [question](https://forums.xilinx.com/t5/Vivado-TCL-Community/lt-list-param-gt-not-listing-a-lot/td-p/998194) in Xilinx\'s forum, but received a reply from Xilinx saying those are internal and thou shall not use it.\\nWhen using the `place.debugShape` in my scripts, I saw some outputs:\\n\\n```\\nShape builder is called from:\\nStack:\\n/opt/Xilinx/Vivado/2018.3/lib/lnx64.o/librdi_place.so(HAPLPlacerShapeBuilder::buildShapes(HDPLNewShapeDB&, HSTPtrHashSet&, HDPLTask&, HAPLPlaceApi*, HDPLControlSetDB const*,\\n/opt/Xilinx/Vivado/2018.3/lib/lnx64.o/librdi_implflow.so(HAPLFFastFlow3::place(HAPLFFastFlowParam const&, HAPLFMigPblockInfo const*)+0x12cf) [0x7f51a8e7bb3f]\\n/opt/Xilinx/Vivado/2018.3/lib/lnx64.o/libtcl8.5.so(+0x334af) [0x7f51ccef74af]\\n```\\nwhich looked like a familiar pattern - a frontend (tcl) binding to a C++ backend. So I was like, ok, may be if I grep the object dump of one of these `.so`, I could get the params? That didn\'t work. But what did work was, when I treated the `.so` in grep as a text file and searched for a \\"something-dot-camelCase\\" pattern, and voila, everything was in plain sight:\\n```\\nplace.debugCongestion\\nplace.debugCrash\\nplace.debugFFGroup\\nplace.debugLightTimer\\nplace.debugMacroInterleavingOptimization\\nplace.debugShape\\nplace.debugShapeAppend\\nplace.debugWireLen\\n...\\n```\\nAnd many more such as:\\n```\\nlogicopt.allowEmptyHierCellNets\\nlogicopt.allowEmptyHierCells\\nlogicopt.allowInverterPushing\\nlogicopt.annotateModifiedPrims\\nlogicopt.applyFinishingTouch\\nlogicopt.applyLogicProp\\nlogicopt.applyPostPwroptCleanup\\nlogicopt.applyRestruct\\n...\\n```\\n\\nSimilarly, there are tuneable params that look like `synth.*`, `route.*`, `power.*`, `timing.*`. in their respective `librdi_*.so`. That\'s all for this post. May be if you have a use-case, where you wanted vivado to do something, but it\'s not visible, one of these parameters might come into use! Of course I get Xilinx\'s point about these being internal knobs ;).\\n\\nFYI following is the command with regex I used:\\n```\\ngrep -a -o -E \'[a-zA-Z0-9]{3,}.[a-z]([a-z0-9][A-Z][A-Z0-9][a-z]|[A-Z0-9][a-z][a-z0-9][A-Z])[a-ZA-Z0-9]{3,}\' /opt/Xilinx/Vivado/2018.3/lib/lnx64.o/librdi_place.so\\n```\\n\\nYou can find about 5000 of the params I scraped, in this repository: https://github.com/syed-ahmed/vivado-hacks"},{"id":"docker-workflow","metadata":{"permalink":"/blog/docker-workflow","editUrl":"https://github.com/syed-ahmed/syed-ahmed.github.io/blob/main/website/blog/2017-06-24-software-development-workflow-with-docker/index.md?plain=1","source":"@site/blog/2017-06-24-software-development-workflow-with-docker/index.md","title":"Software Development Workflow with Docker","description":"To best describe the motivation for writing this post let me describe the perspectives from two different people:","date":"2017-06-24T00:00:00.000Z","formattedDate":"June 24, 2017","tags":[{"label":"docker","permalink":"/blog/tags/docker"},{"label":"software development","permalink":"/blog/tags/software-development"}],"readingTime":9.165,"truncated":false,"authors":[{"name":"Syed Tousif Ahmed","title":"Software Engineer","url":"https://syed.sh","imageURL":"https://github.com/syed-ahmed/syed-ahmed.github.io/raw/main/website/static/img/profile-pic1.jpg","key":"syed"}],"frontMatter":{"slug":"docker-workflow","title":"Software Development Workflow with Docker","authors":"syed","tags":["docker","software development"]},"prevItem":{"title":"Use All Tunable Vivado Internal Params","permalink":"/blog/vivado-internal-params"}},"content":"To best describe the motivation for writing this post let me describe the perspectives from two different people:\\n\\n**The Scientist\'s Perspective:**\\n\\n_I am a scientist working on my next deep learning algorithm that\'s gonna make everyone go like whaaa? Sometimes, I write my algorithm in TensorFlow, sometimes in Torch and sometimes in Caffe. I am also poorly funded and can\'t afford multiple machines for different setups. My problem is that, all these machine learning frameworks are not in sync and some uses different CUDA versions, drivers, modules etc. than the other. Although I can probably make them work in the same machine by being disciplined and having good practices on putting library paths and symlinks and stuff, I will probably screw things up when it comes to update my system/framework. Also, I don\'t wanna write installation instructions of all my packages when I release my code to the conference I\'m submitting my paper to. I want the scientific community to reproduce my experiment exactly as I did it in my machine. Is there something that can streamline this process?_\\n\\n**The Software Developer\'s Perspective:**\\n\\n_I am a software developer who\'s sick and tired of installing stuff and just want to run this freaking code and get on with it. Is there something that can streamline this process?_\\n\\nThe answer to their question is, yes. There is a thing called Docker which can streamline this process. Docker is kinda like a virtual machine but it doesn\'t include the OS, only the settings and minimal stuff that is required to reproduce your setup in another machine. For more on what it is, just read about it in this link: [https://www.docker.com/what-docker](https://www.docker.com/what-docker). Following is my migration process to docker:\\n\\n![](./assets/previous-workflow.png)Above is my previous workflow. I have a computer with all the packages locally installed. I have a text editor where I write and edit my source code. And I have a terminal to compile and run the source code. The problem comes when I have conflicting packages for different kinds of source code I\'m working on. For instance, in my work, some source codes are dependent on specific versions of CUDA. Our CS Java class at RIT one semester was in java version 1.7 and the next semester in java 1.8. Some people have moved to python 3 and some are still using python 2. Hence, in the above workflow, avoiding such conflicts is messy. Following is my current workflow:![](./assets/current-workflow.png)In this workflow, I have a computer with only Docker Engine and universal drivers such as the display driver, installed. The only difference now is that I don\'t have all the packages such as python 2/python 3 locally installed, but installed in Docker Containers. I have different docker containers for different development projects. For instance, the Docker container for TensorFlow has python 2 by default, CUDA 8 and other TensorFlow specific packages installed, whereas the Docker container for Caffe has it\'s own specification. Hence, after I write my code in the text editor locally, I run that code in it\'s specific docker container.\\n\\nFollowing is a step by step process of how I write code in TensorFlow. You can adopt the same process for whatever project you are working on:\\n\\n**Step 1: Edit the code**\\n\\nSo for instance, here is a code for a model that I\'m writing in Visual Studio Code. You might be writing some code for finding the solutions of some differential equations with Runge-Kutta solver in C. Write that code first in an editor. I am still not settled on a single editor. People use Vim, Emacs, Spacemacs, Visual Studio Code, Atom, Sublime, etc... the list goes on.![](./assets/screenshot1.png)\\n\\n**Step 2: Get the Docker Container**\\n\\nAssuming you have installed docker engine in your machine \\\\([https://docs.docker.com/engine/installation/](https://docs.docker.com/engine/installation/)\\\\), now you have to get a docker container. A docker container is an image. You can either build this image from the scratch \\\\(which I\'ll describe in a different post. You can even use multiple images and stack them up and make a single image to your requirements\\\\) or you can get this image from a repository \\\\([https://store.docker.com/search?q=&source=verified&type=image](https://store.docker.com/search?q=&source=verified&type=image)\\\\). For now, I\'ll get the TensorFlow image with the following command in a terminal:\\n\\n```bash\\ndocker run -it gcr.io/tensorflow/tensorflow:latest-devel bash\\n```\\n\\nor \\\\(this command will just pull it, but not run it\\\\)\\n\\n```bash\\ndocker pull gcr.io/tensorflow/tensorflow:latest-devel\\n```\\n\\nThe command pulls the image from the repository and puts it locally in your machine and runs a bash shell in it as follows:\\n\\n```bash\\nSyeds-MBP:ptb luna$ docker run -it gcr.io/tensorflow/tensorflow:latest-devel bash\\nUnable to find image \'gcr.io/tensorflow/tensorflow:latest-devel\' locally\\nlatest-devel: Pulling from tensorflow/tensorflow\\n892cc5bfc051: Already exists \\nf3eda43ea55a: Already exists \\n646005d97ff4: Already exists \\n44d770c1f7bd: Already exists \\n1ce0c4bfe746: Already exists \\nb98b28b649a2: Pull complete \\ndbcb14adf008: Pull complete \\n18aa19fe8596: Pull complete \\n7882fa001225: Pull complete \\ncde898c339f7: Pull complete \\n05db1985de18: Pull complete \\n3ede8b995134: Pull complete \\n7e09e41c9d96: Pull complete \\nc3c542fce062: Pull complete \\n2c4b672418c8: Pull complete \\nDigest: sha256:2b3ab8cbcdd08fb93fc3b4e821ea366c2155a5af9441acfc03f927071b202a19\\nStatus: Downloaded newer image for gcr.io/tensorflow/tensorflow:latest-devel\\nroot@2bef7283910f:~# python\\n```\\n\\nIf you now open and type python or give some other commands, you can see what is installed in this container For instance, in the following, you can see the TensorFlow container start python 2 by default, has TensorFlow version 1.2.0, has bazel 0.4.5 installed, java 1.8 and g++ 5.4.0 compiler. **And you had to copy-paste only one command to get all of those! **That sure solved some of the installation problems our two subjects were having :\\\\)\\n\\n```python\\nroot@2bef7283910f:~# python\\nPython 2.7.12 (default, Nov 19 2016, 06:48:10) \\n[GCC 5.4.0 20160609] on linux2\\nType \\"help\\", \\"copyright\\", \\"credits\\" or \\"license\\" for more information.\\n>>> import tensorflow as tf\\n>>> tf.__version__\\n\'1.2.0\'\\n>>> exit()\\nroot@2bef7283910f:~# bazel version\\nBuild label: 0.4.5\\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\\nBuild time: Thu Mar 16 12:19:38 2017 (1489666778)\\nBuild timestamp: 1489666778\\nBuild timestamp as int: 1489666778\\nroot@2bef7283910f:~# javac -version\\njavac 1.8.0_131\\nroot@2bef7283910f:~# g++ -v\\nUsing built-in specs.\\nCOLLECT_GCC=g++\\nCOLLECT_LTO_WRAPPER=/usr/lib/gcc/x86_64-linux-gnu/5/lto-wrapper\\nTarget: x86_64-linux-gnu\\nConfigured with: ../src/configure -v --with-pkgversion=\'Ubuntu 5.4.0-6ubuntu1~16.04.4\' --with-bugurl=file:///usr/share/doc/gcc-5/README.Bugs --enable-languages=c,ada,c++,java,go,d,fortran,objc,obj-c++ --prefix=/usr --program-suffix=-5 --enable-shared --enable-linker-build-id --libexecdir=/usr/lib --without-included-gettext --enable-threads=posix --libdir=/usr/lib --enable-nls --with-sysroot=/ --enable-clocale=gnu --enable-libstdcxx-debug --enable-libstdcxx-time=yes --with-default-libstdcxx-abi=new --enable-gnu-unique-object --disable-vtable-verify --enable-libmpx --enable-plugin --with-system-zlib --disable-browser-plugin --enable-java-awt=gtk --enable-gtk-cairo --with-java-home=/usr/lib/jvm/java-1.5.0-gcj-5-amd64/jre --enable-java-home --with-jvm-root-dir=/usr/lib/jvm/java-1.5.0-gcj-5-amd64 --with-jvm-jar-dir=/usr/lib/jvm-exports/java-1.5.0-gcj-5-amd64 --with-arch-directory=amd64 --with-ecj-jar=/usr/share/java/eclipse-ecj.jar --enable-objc-gc --enable-multiarch --disable-werror --with-arch-32=i686 --with-abi=m64 --with-multilib-list=m32,m64,mx32 --enable-multilib --with-tune=generic --enable-checking=release --build=x86_64-linux-gnu --host=x86_64-linux-gnu --target=x86_64-linux-gnu\\nThread model: posix\\ngcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.4)\\n```\\n\\nBut now you might be saying, I don\'t see my code in here. How do I get my code in this container? So the answer lies in how we issued the docker run command. Since this is a container and is a closed system, we need to **mount** our local folder into this container. Similarly, if we want to do some networking stuff, we would have to open the port in the local machine to this container  with the docker run command. You can learn all the options that can be passed to docker run command in here: [https://docs.docker.com/engine/reference/run/](https://docs.docker.com/engine/reference/run/). For now type **exit** and go to step 3.\\n\\n**Step 3: Running Docker**\\n\\nTo run a docker container, you need to identify which docker container to run. You can find out about available docker containers in your machine using the following command:\\n\\n```bash\\nSyeds-MBP:ptb luna$ docker images\\nREPOSITORY                     TAG                 IMAGE ID            CREATED             SIZE\\ngcr.io/tensorflow/tensorflow   latest-devel        61c8843c7545        9 days ago          1.78 GB\\n```\\n\\nYou can see that our docker container has a repository name, tag, image ID. You can run this container as follows:\\n\\n```bash\\ndocker run -it gcr.io/tensorflow/tensorflow:latest-devel bash\\n```\\n\\nor with directly with the image id.\\n\\n```bash\\ndocker run -it 61c8843c7545 bash\\n```\\n\\nBut our run command is not complete yet. We need to expose port 6006. So let\'s do that by adding `-p HostPort:ContainerPort` argument as follows:\\n\\n```bash\\ndocker run -it -p 6006:6006 61c8843c7545 bash\\n```\\n\\nNow we need to mount our source code directory with a `-v SourceDirectory:ContainerDirectory` argument. Hence, our final run command is as follows:\\n\\n```bash\\nSyeds-MBP:ptb luna$ docker run -it -p 6006:6006 -v /Users/luna/workspace/models/:/root/workspace 61c8843c7545 bash\\nroot@89ce92fb9880:~# ls\\nworkspace\\nroot@89ce92fb9880:~# cd workspace/\\nroot@89ce92fb9880:~/workspace# ls\\nAUTHORS            WORKSPACE           cognitive_mapping_and_planning  inception                         neural_gpu             resnet         swivel       tutorials\\nCONTRIBUTING.md    adversarial_crypto  compression                     learning_to_remember_rare_events  neural_programmer      setup.py       syntaxnet    video_prediction\\nISSUE_TEMPLATE.md  adversarial_text    differential_privacy            lfads                             next_frame_prediction  skip_thoughts  tags\\nLICENSE            attention_ocr       domain_adaptation               lm_1b                             object_detection       slim           textsum\\nREADME.md          autoencoder         im2txt                          namignizer                        real_nvp               street         transformer\\nroot@89ce92fb9880:~/workspace#\\n```\\n\\nNow you can see that your local source folder is mounted in a folder inside the container. Since, the local folder is mounted, any change you make in the local folder is going to reflect inside the container and any change you make inside the container is going to reflect in the local folder. Just beware of changing permission and doing git push/pull inside the container as this might screw things up in your workflow. Just keep things clean and edit the source code locally, do git commit/pull/push locally and only run and compile code in this development workflow \\\\(the next post will talk about building docker images and how you can build on top of this image and add features to this image\\\\).\\n\\n**Step 4: Run the source code in the docker container**\\n\\nAssuming you have edited the source code, let\'s run the code in the docker container as follows. I\'m going to run the ptb word model as follows:\\n\\n```bash\\nroot@89ce92fb9880:~/workspace/tutorials/rnn/ptb\xa3 python ptb_word_lm.py --data_path=simple-examples/data/ --model=small --save_path=./  \\n2017-06-24 09:53:40.418532: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn\'t compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\\n2017-06-24 09:53:40.418576: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn\'t compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\\n2017-06-24 09:53:40.418586: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn\'t compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\\n2017-06-24 09:53:40.419125: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn\'t compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\\n2017-06-24 09:53:40.419173: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn\'t compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\\nEpoch: 1 Learning rate: 1.000\\n0.004 perplexity: 7284.361 speed: 913 wps\\n0.104 perplexity: 856.644 speed: 1090 wps\\n```\\n\\nNow let\'s make use of that exposed port using the following command and browsing to [https://localhost:6006](https://localhost:6006) in our local browser:\\n\\n```bash\\nroot@89ce92fb9880:~/workspace/tutorials/rnn/ptb\xa3 tensorboard --logdir=./\\nStarting TensorBoard 54 at https://89ce92fb9880:6006\\n(Press CTRL+C to quit)\\n```\\n\\nAs you can see, you got tensorboard opened up through port 6006:  \\n![](./assets/screenshot2.png)\\n\\n**Conclusion**\\n\\nSo that\'s it. Edit the code locally, build and run in the container, and save experiment results from the container to the host. I have specifically used a development workflow from deep learning, but you can adopt this workflow for any kind of development. For instance, if you are doing web development with React or Angular, you can pull a container which has the required dependency like node, babel etc. and write you source code locally and run it in the container. Same goes for scientists who are writing high performance computing code. You can create a container with all the required packages and do the same. In the next post, I\'ll talk about how you can build your custom container. For now, this is it."}]}')}}]);